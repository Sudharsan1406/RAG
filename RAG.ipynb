{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d883f33e-bbc2-4ba4-a78e-4484e96771ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\91968\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: openai in c:\\users\\91968\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\91968\\anaconda3\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\91968\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\91968\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\91968\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\91968\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\91968\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91968\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\91968\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pdfplumber) (10.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pdfplumber) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (43.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.36.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: click in c:\\users\\91968\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\91968\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: absl-py in c:\\users\\91968\\anaconda3\\lib\\site-packages (from rouge-score) (2.3.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91968\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91968\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\91968\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91968\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\91968\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu openai pdfplumber sentence-transformers python-dotenv nltk rank-bm25 rouge-score tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4742a8e-ec83-4911-ba3a-41f60beb10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and setup\n",
    "import os, sys, pickle, math\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "DATA_DIR = Path(\"data\")\n",
    "INDEX_DIR = Path(\"index_store\")\n",
    "INDEX_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d314d3d0-ed9d-4109-8f37-32b731c4fb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder path (absolute): C:\\Users\\91968\\OneDrive\\Desktop\\Pthon DS GuVi\\HCL\\RAG\\Data\n",
      "Exists? True\n",
      "\n",
      "Files found in data : 2\n",
      "1. Doc.pdf  (size: 2517794 bytes)\n",
      "2. OPENAI_API_KEY.env  (size: 179 bytes)\n"
     ]
    }
   ],
   "source": [
    "# verify data folder and list files\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "print(\"Data folder path (absolute):\", DATA_DIR.resolve())\n",
    "print(\"Exists?\", DATA_DIR.exists())\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    print(\"\\n--> The folder 'data' does not exist. Create it and add your PDF/TXT files there.\")\n",
    "else:\n",
    "    files = sorted([p for p in DATA_DIR.iterdir() if p.is_file()])\n",
    "    print(f\"\\nFiles found in {DATA_DIR} : {len(files)}\")\n",
    "    for i, p in enumerate(files, 1):\n",
    "        print(f\"{i}. {p.name}  (size: {p.stat().st_size} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a3ff82-fdc8-42e5-9b4c-3331ddf7738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 2 documents.\n",
      "Documents with EMPTY text: 0\n",
      "\n",
      "--- Doc.pdf  (chars: 10454) ---\n",
      "DATA SCIENCE Interview Questions Mukesh Sablani Data Analytics & AI Coach ğ‘ğğšğğ² ğ­ğ¨ ğ¥ğšğ§ğ ğ²ğ¨ğ®ğ« ğğ«ğğšğ¦ ğƒğšğ­ğš ğ€ğ§ğšğ¥ğ²ğ¬ğ­ & ğ€ğˆ ğ«ğ¨ğ¥ğ? Mukesh Sablani Data Analytics & AI Coach What is Data Science? How is it different from 1 Data Analytics and Data Engineering? Data Science combines statistics, programming, and machine learning to extract meaningful insights and predictions from data. Data Analytics primarily\n",
      "\n",
      "\n",
      "--- OPENAI_API_KEY.env  (chars: 179) ---\n",
      "OPENAI_API_KEY=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\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load text from PDF, show summary info\n",
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "docs = []   # will hold dicts: {\"id\": filename, \"text\": content}\n",
    "\n",
    "def load_txt_file(path: Path) -> str:\n",
    "    try:\n",
    "        return path.read_text(encoding='utf-8', errors='ignore')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading txt {path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_pdf_file(path: Path) -> str:\n",
    "    text_parts = []\n",
    "    try:\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for pnum, page in enumerate(pdf.pages, start=1):\n",
    "                try:\n",
    "                    t = page.extract_text()\n",
    "                    if t:\n",
    "                        text_parts.append(t)\n",
    "                except Exception as ex:\n",
    "                    print(f\"  Warning: page {pnum} parse error in {path.name}: {ex}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PDF {path.name}: {e}\")\n",
    "        # If PDF is scanned image-only, pdfplumber will return empty -> need OCR (pytesseract)\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "# iterate files\n",
    "for p in sorted(DATA_DIR.iterdir()):\n",
    "    if not p.is_file():\n",
    "        continue\n",
    "    suffix = p.suffix.lower()\n",
    "    txt = \"\"\n",
    "    if suffix == \".pdf\":\n",
    "        txt = load_pdf_file(p)\n",
    "    elif suffix in {\".txt\", \".md\"}:\n",
    "        txt = load_txt_file(p)\n",
    "    else:\n",
    "        # try reading as text fallback\n",
    "        try:\n",
    "            txt = load_txt_file(p)\n",
    "        except:\n",
    "            txt = \"\"\n",
    "    docs.append({\"id\": p.name, \"text\": txt})\n",
    "\n",
    "# Quick checks / report\n",
    "print(f\"\\nLoaded {len(docs)} documents.\")\n",
    "empty_count = sum(1 for d in docs if not d['text'].strip())\n",
    "print(f\"Documents with EMPTY text: {empty_count}\")\n",
    "\n",
    "# show a short preview for each doc\n",
    "for d in docs:\n",
    "    L = len(d['text'])\n",
    "    preview = d['text'][:400].replace(\"\\n\", \" \") if L>0 else \"<EMPTY>\"\n",
    "    print(f\"\\n--- {d['id']}  (chars: {L}) ---\\n{preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a987f6-902a-46d9-b076-98fa77b52198",
   "metadata": {},
   "source": [
    "# Chunk the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c25447a-3e30-4cbf-8bf9-1e297409ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 14\n",
      "Example chunk preview:\n",
      " DATA SCIENCE\n",
      "Interview Questions\n",
      "Mukesh Sablani\n",
      "Data Analytics & AI Coach\n",
      "ğ‘ğğšğğ² ğ­ğ¨ ğ¥ğšğ§ğ ğ²ğ¨ğ®ğ« ğğ«ğğšğ¦ ğƒğšğ­ğš ğ€ğ§ğšğ¥ğ²ğ¬ğ­ & ğ€ğˆ ğ«ğ¨ğ¥ğ?\n",
      "Mukesh Sablani\n",
      "Data Analytics & AI Coach\n",
      "What is Data Science? How is it different from\n",
      "1\n",
      "Data Analytics and Data Engineering?\n",
      "Data Science combines statistics, programming, and\n"
     ]
    }
   ],
   "source": [
    "# Chunk the document into smaller pieces\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    idx = 0\n",
    "\n",
    "    while start < L:\n",
    "        end = min(start + chunk_size, L)\n",
    "        piece = text[start:end].strip()\n",
    "\n",
    "        if piece:\n",
    "            chunk_id = hashlib.md5((str(idx) + piece[:50]).encode()).hexdigest()\n",
    "            chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": piece\n",
    "            })\n",
    "\n",
    "        idx += 1\n",
    "        if end >= L:\n",
    "            break\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Apply chunking to all docs\n",
    "all_chunks = []\n",
    "metadatas = []\n",
    "\n",
    "for d in docs:\n",
    "    chunks = chunk_text(d['text'], chunk_size=1000, overlap=200)\n",
    "    for c in chunks:\n",
    "        all_chunks.append(c['text'])\n",
    "        metadatas.append({\n",
    "            \"source\": d['id'],\n",
    "            \"chunk_id\": c['chunk_id'],\n",
    "            \"text\": c['text']\n",
    "        })\n",
    "\n",
    "print(\"Total chunks created:\", len(all_chunks))\n",
    "print(\"Example chunk preview:\\n\", all_chunks[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad4439-7fbb-475b-bcd7-e6557bd2180f",
   "metadata": {},
   "source": [
    "# Generate Embeddings (sentence-transformers, no API needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "671150b0-5a31-46a1-8017-48298646647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91968\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\91968\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acd87bf51214ce194ebb86110d3c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (14, 384)\n",
      "Example vector (first 5 dims): [ 0.00397044 -0.01175615  0.03576324  0.03249187  0.01963547]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a lightweight embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "vectors = model.encode(all_chunks, show_progress_bar=True)\n",
    "\n",
    "# Convert to numpy array\n",
    "xb = np.array(vectors).astype('float32')\n",
    "\n",
    "print(\"Embedding shape:\", xb.shape)   # Should be (13, 384)\n",
    "print(\"Example vector (first 5 dims):\", xb[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa021a4-2f70-4b7a-9f6b-4bc9df99a435",
   "metadata": {},
   "source": [
    "# Build the FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eefa002a-6e27-4ac7-ba44-7554cc29f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created.\n",
      "Total vectors indexed: 14\n",
      "Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# xb is your embedding array from previous step (shape: 13, 384)\n",
    "dim = xb.shape[1]  # embedding dimension\n",
    "\n",
    "index = faiss.IndexFlatL2(dim)  # simple L2 distance index\n",
    "index.add(xb)                   # add all embeddings\n",
    "\n",
    "# Save index and metadata\n",
    "faiss.write_index(index, str(INDEX_DIR / \"faiss.idx\"))\n",
    "\n",
    "with open(INDEX_DIR / \"metadatas.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadatas, f)\n",
    "\n",
    "print(\"FAISS index created.\")\n",
    "print(\"Total vectors indexed:\", index.ntotal)\n",
    "print(\"Dimension:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcde906-9a2f-49cd-8b98-82b76315c231",
   "metadata": {},
   "source": [
    "# Build the Search Functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f55d5f72-37b1-466a-a0e0-654c738061ab",
   "metadata": {},
   "source": [
    "Weâ€™ll implement two search modes:\n",
    "\n",
    "FAISS Search â†’ vector similarity\n",
    "\n",
    "BM25 Search â†’ keyword relevance\n",
    "\n",
    "Hybrid Search â†’ best of both (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61e54d5c-1617-4b60-94df-2f0ccb720fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "1. Source: Doc.pdf\n",
      "DATA SCIENCE\n",
      "Interview Questions\n",
      "Mukesh Sablani\n",
      "Data Analytics & AI Coach\n",
      "ğ‘ğğšğğ² ğ­ğ¨ ğ¥ğšğ§ğ ğ²ğ¨ğ®ğ« ğğ«ğğšğ¦ ğƒğšğ­ğš ğ€ğ§ğšğ¥ğ²ğ¬ğ­ & ğ€ğˆ ğ«ğ¨ğ¥ğ?\n",
      "Mukesh Sablani\n",
      "Data Analytics & AI Coach\n",
      "What is Data Science? How is it diff \n",
      "\n",
      "2. Source: Doc.pdf\n",
      "ices or temperatures.\n",
      "Classification problems involve predicting discrete classes or\n",
      "categories, like determining whether an email is spam or not spam.\n",
      "Different algorithms and evaluation metrics are  \n",
      "\n",
      "3. Source: Doc.pdf\n",
      "s gender,\n",
      "color, or brand names.\n",
      "Quantitative data consists of numbers and can be further\n",
      "categorized as discrete (countable values like number of items) or\n",
      "continuous (measurable values like height o \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91968\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load chunks again (if needed)\n",
    "all_texts = all_chunks  # list of chunk strings\n",
    "\n",
    "# Prepare BM25 tokenization\n",
    "tokenized = [word_tokenize(t.lower()) for t in all_texts]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "\n",
    "# 1ï¸âƒ£ FAISS search\n",
    "def faiss_search(query, k=5):\n",
    "    qvec = model.encode([query])[0].astype('float32')\n",
    "    D, I = index.search(np.array([qvec]), k)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        results.append(metadatas[idx])\n",
    "    return results\n",
    "\n",
    "\n",
    "# 2ï¸âƒ£ BM25 search\n",
    "def bm25_search(query, k=5):\n",
    "    tokens = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    top_idx = list(np.argsort(scores)[::-1][:k])\n",
    "    return [metadatas[i] for i in top_idx]\n",
    "\n",
    "\n",
    "# 3ï¸âƒ£ Hybrid search (best of both)\n",
    "def hybrid_search(query, k=5):\n",
    "    faiss_res = faiss_search(query, k)\n",
    "    bm25_res = bm25_search(query, k)\n",
    "\n",
    "    seen = set()\n",
    "    combined = []\n",
    "\n",
    "    # merge unique\n",
    "    for item in faiss_res + bm25_res:\n",
    "        cid = item[\"chunk_id\"]\n",
    "        if cid not in seen:\n",
    "            combined.append(item)\n",
    "            seen.add(cid)\n",
    "        if len(combined) >= k:\n",
    "            break\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "# ğŸ” Test search\n",
    "query = \"What is data science?\"\n",
    "results = hybrid_search(query, k=3)\n",
    "\n",
    "print(\"Search Results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. Source: {r['source']}\")\n",
    "    print(r['text'][:200], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfaf52-e4c1-4838-bffc-e5148102b0d6",
   "metadata": {},
   "source": [
    "# Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f4ab0f-b32d-40df-95d4-c28af8990bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY ===\n",
      "ices or temperatures. Classification problems involve predicting discrete classes or\n",
      "categories, like determining whether an email is spam or not spam. Different algorithms and evaluation metrics are used suited to the\n",
      "nature of the prediction problem. What programming languages and tools\n",
      "14\n",
      "are you familiar with for data science? Python and R are widely used for a variety of data manipulation,\n",
      "statistical analysis, visualization, and machine learning tasks due to\n",
      "their rich ecosystems and community support.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91968\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def summarize_local(docs, length=\"medium\"):\n",
    "    \"\"\"\n",
    "    docs = list of metadata objects with 'text'\n",
    "    length options: short, medium, long\n",
    "    \"\"\"\n",
    "    all_sentences = []\n",
    "    for d in docs:\n",
    "        all_sentences.extend(sent_tokenize(d[\"text\"]))\n",
    "\n",
    "    # choose number of sentences\n",
    "    n = {\"short\": 2, \"medium\": 5, \"long\": 10}.get(length, 5)\n",
    "\n",
    "    # return first N sentences (simple but works well for structured docs)\n",
    "    return \" \".join(all_sentences[:n])\n",
    "\n",
    "\n",
    "# Test summarization using search results\n",
    "query = \"Explain data science\"\n",
    "results = hybrid_search(query, k=5)\n",
    "\n",
    "summary = summarize_local(results, length=\"medium\")\n",
    "\n",
    "print(\"=== SUMMARY ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bc93d-fec4-4f90-9719-d5c2511ff2d5",
   "metadata": {},
   "source": [
    "# OpenAI Summarizer (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d54c6d9-ae94-4ec8-9d50-79ad27ea56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\91968\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\91968\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\91968\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\91968\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4718023-703a-46d6-a434-3973c74f304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: True\n"
     ]
    }
   ],
   "source": [
    "# verify .env loads and key is visible to Python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(\"OPENAI_API_KEY:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "# prints True if key loaded; prints False if not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f09d69-28c2-46b3-9adb-3752637881a8",
   "metadata": {},
   "source": [
    "# OpenAI summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78eba21b-9a83-42d4-9146-36656674f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key OK\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_KEY is None:\n",
    "    print(\"Warning: OPENAI_API_KEY not set. OpenAI summarizer will not work until key is set.\")\n",
    "else:\n",
    "    print(\"OpenAI key OK\")\n",
    "\n",
    "# initialize client only if key present\n",
    "client = OpenAI(api_key=OPENAI_KEY) if OPENAI_KEY else None\n",
    "\n",
    "# Local fallback summarizer (simple extractive)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def summarize_local(docs, length=\"medium\"):\n",
    "    all_sentences = []\n",
    "    for d in docs:\n",
    "        all_sentences.extend(sent_tokenize(d.get(\"text\",\"\")))\n",
    "    n = {\"short\":2, \"medium\":5, \"long\":10}.get(length,5)\n",
    "    return \" \".join(all_sentences[:n])\n",
    "\n",
    "# Build prompt while keeping it short: limit total characters to avoid hitting token limits\n",
    "def make_prompt_safe(docs, length=\"medium\", char_limit=3500):\n",
    "    header = f\"Summarize the following retrieved passages into a {length} summary.\\n\"\n",
    "    header += \"Use only the information provided. Don't add facts not present. Keep it concise.\\n\\n\"\n",
    "    body = \"\"\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        chunk_text = d.get(\"text\",\"\").strip()\n",
    "        entry = f\"### PASSAGE {i} (Source: {d.get('source','unknown')}):\\n{chunk_text}\\n\\n\"\n",
    "        if len(body) + len(entry) > char_limit:\n",
    "            break\n",
    "        body += entry\n",
    "    footer = \"\\n### Instructions:\\n- Do not hallucinate.\\n- Produce a single coherent paragraph summary.\\n\"\n",
    "    prompt = header + body + footer\n",
    "    # ensure prompt not empty\n",
    "    return prompt if body.strip() else header + \"No text available.\"\n",
    "\n",
    "def summarize_openai(docs, length=\"medium\", model=\"gpt-4o-mini\", max_tokens=450, retry=1):\n",
    "    \"\"\"\n",
    "    docs: list of {'text':..., 'source':...}\n",
    "    Returns: string summary (or local fallback on error)\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        return summarize_local(docs, length)\n",
    "    prompt = make_prompt_safe(docs, length=length)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\":\"system\", \"content\":\"You are a concise summarization assistant.\"},\n",
    "                {\"role\":\"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        # new client returns choices list\n",
    "        summary = resp.choices[0].message.content\n",
    "        return summary.strip()\n",
    "    except Exception as e:\n",
    "        # retry once for transient errors, then fallback\n",
    "        print(\"OpenAI API error:\", str(e))\n",
    "        if retry > 0:\n",
    "            time.sleep(1)\n",
    "            return summarize_openai(docs, length, model, max_tokens, retry-1)\n",
    "        print(\"Falling back to local summarizer.\")\n",
    "        return summarize_local(docs, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df33db1-56fc-43d2-afa9-ff78def1f1ba",
   "metadata": {},
   "source": [
    "# Replace rag_answer to use OpenAI summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a653b3e-8b89-4606-9cb1-d97e7e8c949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "Mukesh Sablani, a Data Analytics and AI Coach, explains that Data Science integrates statistics, programming, and machine learning to derive insights from data, while Data Analytics focuses on interpreting existing data for decision-making, and Data Engineering builds the infrastructure for data processing. He categorizes data into qualitative (categories) and quantitative (numbers), with further distinctions into discrete and continuous types. He also highlights the importance of programming languages like Python and R, SQL for data management, and visualization tools such as Tableau and Power BI. Additionally, he differentiates between structured (organized in tables), semi-structured (lacks strict format but includes tags), and unstructured data (no predefined format).\n",
      "\n",
      "=== Sources ===\n",
      "- Doc.pdf\n",
      "- Doc.pdf\n",
      "- Doc.pdf\n",
      "- Doc.pdf\n",
      "- Doc.pdf\n"
     ]
    }
   ],
   "source": [
    "# Final RAG function using OpenAI summarizer\n",
    "def rag_answer_openai(query, k=5, length=\"medium\"):\n",
    "    # hybrid_search(query, k) must be available from your previous cells\n",
    "    retrieved = hybrid_search(query, k=k)   # expects list of metadatas {'text', 'source', ...}\n",
    "    summary = summarize_openai(retrieved, length=length)\n",
    "    return summary, retrieved\n",
    "\n",
    "# quick test (replace query with suitable text)\n",
    "q = \"What is data science?\"\n",
    "s, srcs = rag_answer_openai(q, k=5, length=\"short\")\n",
    "print(\"=== Summary ===\")\n",
    "print(s)\n",
    "print(\"\\n=== Sources ===\")\n",
    "for r in srcs:\n",
    "    print(\"-\", r.get(\"source\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
